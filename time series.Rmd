---
title: "Mean Temperature Time Series Forecasting"
author: "Valencia Lie"
date: "01/08/2020"
output: 
  prettydoc::html_pretty:
    theme: hpstr
    highlight: github
    toc: true
    number_sections: true
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
In this report, I will forecast future data about New Delhi's mean temperature based on past data using time series.

# Structure of this report
- Read data and pre-processing
- Exploratory Data Analysis 
- Time Series object and decomposition
- Modelling and forecast
  - SMA
  - SES, DES, TES
  - ARIMA
  - SARIMA
  - STLM
- Evaluation of model and comparison:
  - RMSE, MAPE 
  - Violation of Asumptions:
    - No autocorrelation between errors
    - Normality of error
- Tuning and conclusion

# Read data and pre-processing
```{r warning=FALSE, message=FALSE}
library(tidyverse)
train <- read_csv("DailyDelhiClimateTrain.csv")
test <- read_csv("DailyDelhiClimateTest.csv")

combine <- rbind(train, test)
```

```{r}
glimpse(combine)
anyNA(combine)
summary(combine)

combine$monthyr <- format(as.Date(combine$date), "%Y-%m")
```

From the summary for both the combined dataset, we can tell that there is no missing dates (consecutive dates in time series forecasting is incredibly important!) because the mean and median of the dates are exactly the same. 

However, since the computation is too heavy, we will just extract the month and year of the dates in the dataset.

# Exploratory Data Analysis 

We will first do some visualisation to the mean temp data in the dataset to get a feel of the trend and seasonality.
```{r fig.height=10}
library(ggplot2)

combine_new <- combine %>% 
  group_by(monthyr) %>% 
  summarise(average = mean(meantemp))

ggplot(data = combine_new, mapping = aes(x = monthyr, y = average)) +
  geom_line(aes(group = 1)) +
  theme_minimal()
```

From the above visualisation, we can easily tell that there is seasonality (repeated pattern every year) and that it is of the type additive and not multiplicative.

Logically, this pattern makes sense. This is because New Delhi experiences monsoon every June-September, which causes the mean temperature to dip in these few months. Once the monsoon ends, the mean temperature will then increase again and peak right before the monsoon season hits. Since this monsoon season is a yearly occurrence (and not once off), this causes the seasonality in the data shown. 

# Time Series object and decomposition

```{r message=FALSE, warning=FALSE}
library(forecast)
combine_ts <- ts(data = combine_new$average, start = c(2013,01), end = c(2017,04), frequency = 12)

combine_ts %>% 
  decompose(type = "additive") %>% 
  autoplot()
```

We can see from the decomposed model that there is actually an upward trend in the mean temperature of New Delhi throughout the years. Again, logically, this makes sense because of climate change that has plagued our world and cause every country's mean temperature to rise exponentially.

# Cross Validation
```{r}
train <- head(combine_ts, -24) 
test <- tail(combine_ts, 24)
```

# Modelling and forecast

## SMA
Simple Moving Average (SMA) is a machine learning algorithm that forecasts future data by simply moving the average of the last n data before the data forecasted. However, since this algorithm is only able to forecast using data that has no trend and seasonality, this algorithm is not suitable to be used on this dataset (which has both trend and seasonality).

## SES, DES, TES
Similar to SMA, Simple Exponential Smoothing (SES) is more suitable for dataset that has no trend and no seasonality. The only difference between SES and SMA is that SES gives different weightage to the data in the dataset (more weightage to newer data than older), whereas SMA gives equal weightage to all data, regardless of how old or new they are (hence how irrelevant or relevant they are to future data prediction). However, with that being said, SES is still not possible to be used for this dataset that has both trend and seasonality.

Double Exponential Smoothing (DES or Holt) works just like SES. It gives different weightage to the data in the dataset. However, similar to SES, this algorithm is not suitable for dataset that has both trend and seasonality as it is only suitable for dataset that only has trend but no seasonality.

Triple Exponential Smoothing (TES or Holt Winters), on the other hand, works well with this dataset because it is suitable for datasets that have both trend and seasonality.

```{r}
train_hw <- HoltWinters(x = train)
```

```{r message=FALSE, warning=FALSE}
library(MLmetrics)
hw_forecast <- forecast(train_hw, h = 24)
MAPE(hw_forecast$mean, test)*100
```

For this particular Holt Winters model, the error of the prediction that the model generates is around 6.49%. For clarity sake, we will try to make a visualisation of how far off (or how reliable) the prediction is to the true data.

```{r fig.height=8, fig.width=10}
train %>% 
  autoplot(series = "actual train") +
  autolayer(test, series = "actual test") +
  autolayer(hw_forecast$mean, series = "predicted")
```
From the visualisation, we can tell that the predictions made by this particular model is quite accurate for the most parts: it predicted the correct dates for the peaks and troughs. However, there is still several errors rendered that nonetheless could still be fixed.

## ARIMA
AutoRegressive Integrated Moving Average (ARIMA) is a powerful algorithm that allows us to forecast future data better and more reliable. It adopts 2 methods: the moving average method that we previously have seen in SMA and also autoregressive method that we have seen in linear regression models before. However, this method does not work well with this particular dataset because ARIMA does not work well with dataset that has a seasonality.

## SARIMA
Hence, to tackle the above problem, we use Seasonal ARIMA (SARIMA). SARIMA adopts the same approach as ARIMA, though it does tackle the seasonality aspect in a different way. It does differencing to the dataset to remove the trend and seasonality of the dataset in order to strip the data to its bare minimum, allowing for easier computation.

```{r message=FALSE, warning=FALSE}
library(tseries)
adf.test(train)
```
H0: Data is not stationary
H1: Data is stationary

First of all, before we do differencing, we will have to do an ADF test on the dataset to see whether the data is stationary enough or not (stationary = no trend and no seasonality). However, according to the above p-value, since the p-value is > 0.05, we reject H1 and accept H0, meaning that we accept that the data is not stationary.

To be very sure, we will conduct KPSS test to know for sure whether the data is stationary or not.

```{r}
kpss.test(train)
```
H0: Data is stationary
H1: Data is not stationary

According to the above p-value, the p-value is above 0.05, making us accept H0 and believe that the data is stationary. This directly contradicts with the conclusion we had with the ADF test. However, because the p-value here is not much bigger than 0.05, whereas the ADF p-value is much bigger than 0.05, I will conclude that it is more reliable to trust the ADF conclusion: the data is not stationary.

Hence, we will have to do differencing to the data.

```{r}
train_diff <- train %>%
  diff(lag = 12) %>% #remove seasonality
  diff(lag = 1) %>% 
  diff(lag = 1) %>% 
  diff(lag = 1) #remove trend
  
adf.test(train_diff)
kpss.test(train_diff)
```
From the above p-values, we can tell that by differencing the data thrice for the ARIMA (trend) and once for SARIMA (seasonality), the data is truly stationary now. We can try to decompose the data that has undergone differencing to see the difference.

```{r}
train_diff %>% 
  autoplot()
```

From the above visualisation, we can tell that the data has been stripped to its bare minimum (without any trend and seasonality). Next, we will proceed with SARIMA.

### Fitting with SARIMA automatically
```{r}
train_auto <- auto.arima(train, seasonal = T)
summary(train_auto)
```

According to the model auto generated, the 'best' SARIMA model is ARIMA(0,0,0)(0,1,0)[12]. However, we will try to compare it if with do the SARIMA model manually.

### Fitting with SARIMA manually
A typical SARIMA model index is ARIMA(p,d,q)(P,D,Q)[frequency].

```{r fig.width=15, fig.height=10}
tsdisplay(train_diff)
```
Based on the above visualisation,
PACF
SARIMA: it is difficult to see whether the PACF spikes at lags of multiples of 12 due to the small scale (only limited to 4) (P)
ARIMA : PACF spikes at lag 1.  (p)

ACF 
SARIMA : it is difficult to see whether the ACF spikes at lags of multiples of 12 due to the small scale (only limited to 4) (Q)
ARIMA : it is difficult to see as well but ACF spikes at around lag 1. (q)

We will hence try to build a model with the index ARIMA(1,3,1)(0,1,0)[12], ARIMA(1,2,1)(0,1,0)[12] and ARIMA(1,1,1)(0,1,0)[12] and compare it with the auto generated model.

```{r}
library(forecast)
train_sarima1 <- Arima(y = train, order = c(1,3,1), seasonal = list(order = c(0,1,0), period = 12))
train_sarima2 <- Arima(y = train, order = c(1,2,1), seasonal = list(order = c(0,1,0), period = 12)) 
train_sarima3 <- Arima(y = train, order = c(1,1,1), seasonal = list(order = c(0,1,0), period = 12)) 
```

```{r}
summary(train_sarima1)
summary(train_sarima2)
summary(train_sarima3)
```

According to the summary of each model, the auto generated SARIMA model has a MAPE of 30.0%. On the other hand, the first manually built SARIMA model has a MAPE of 48.3%, the second manually built SARIMA model has a MAPE of 3.25% and the third manually built SARIMA model has a MAPE of 28.6%.

However, this is only accurate if it is predicting the train dataset. Let's forecast it to the test dataset and see the MAPE of each model.

```{r}
auto_forecast <- forecast(object = train_auto, h = 24)
sarima1_forecast <- forecast(object = train_sarima1, h = 24)
sarima2_forecast <- forecast(object = train_sarima2, h = 24)
sarima3_forecast <- forecast(object = train_sarima3, h = 24)
```

```{r}
MAPE(auto_forecast$mean, test)*100
MAPE(sarima1_forecast$mean, test)*100
MAPE(sarima2_forecast$mean, test)*100
MAPE(sarima3_forecast$mean, test)*100
```
From the above calculation, we can tell that the best model out of the 4 is the third manually built SARIMA model with a MAPE of 6.42%.

```{r fig.height=10, fig.width=8}
train %>% 
  autoplot(series = "actual train") +
  autolayer(test, series = "actual test") +
  autolayer(auto_forecast$mean, series = "auto SARIMA predicted") +
  autolayer(sarima1_forecast$mean, series = "SARIMA 1 predicted")+
  autolayer(sarima2_forecast$mean, series = "SARIMA 2 predicted") +
  autolayer(sarima3_forecast$mean, series = "SARIMA 3 predicted")
```

With the above visualisation, we can easily tell the best SARIMA models out of the four. The third manually built SARIMA model was able to predict future data better than the other two (clearly pinpointing when is the peak mean temperature and when is the trough).

## STLM
STLM is a method of forecasting that combines STL (Seasonal and Trend decomposition using Loess) decomposition with method of forecasting. (exponential smoothing, ARIMA, etc)

However, one very big downside to STLM is that it is unable to process data that is multiplicative. Thankfully, our dataset is additive, so there will be no problem using STLM on our dataset.

```{r fig.height=4, fig.width=8}
train_stlm <- stlm(y = train, method = "ets")
stlm_forecast <- forecast(train_stlm, h = 24)
train %>% 
  autoplot(series = "actual train") +
  autolayer(test, series = "actual test") +
  autolayer(stlm_forecast, series = "STLM predicted")

MAPE(stlm_forecast$mean, test)*100
```

Based on the above calculation, the STLM model generates a MAPE of 7.16%. The visualisation also tells us that the STLM model is generally accurate in forecasting future data (clearly indicating the correct dates for the peaks and troughs of the average temp).

# Evaluation of model and comparison

## MAPE 
Based on the MAPE of all the models, the clear winner is third manually built SARIMA model because it has the least percentage error compared to other models.

## Asumptions

### No autocorrelation between errors
We will try to see whether the third manually SARIMA model is able to fulfill all assumptions.

```{r}
Box.test(train_sarima3$residuals)
```

H0: no-autocorrelation 
H1: autocorrelation 

Since the p-value of the test is more than 0.05. We reject H1 and accept H0. Hence, it can be said that this model has no autocorrelation between errors. 

#### Why do we want no autocorrelation between errors?
We want no autocorrelation between errors because the moment errors have autocorrelation with each other, we should minimise these errors because 1 error lead to another, which can be severely detrimental to our model as well as our forecasting results. 

### Normality of error

```{r}
shapiro.test(train_sarima3$residuals)
```

H0: residuals are distributed normally
H1: residuals not distributed normally

Since the p-value of the test is more than 0.05. We reject H1 and accept H0. Hence, it can be said that the residuals are distributed normally. 

#### Why do we want errors that follow a normal distribution?
When errors of a model follow a standard normal distribution, its mean will be at 0 and the majority of the data of the error will be close to 0, making the model more reliable as the error will statistically be close to 0. Hence, we will try to make sure that the errors of our model follow a close resemblance of a normal distribution.

# Tuning and Conclusion

Since our SARIMA model has a low MAPE and fulfills all the 2 assumptions, no tuning, as of now, is needed. However, with that being said, there may still be improvements that can be made to the model through some fine tunings in order to predict a more reliable and accurate future data. It is also worthy to note that perhaps daily data (where the frequency is 365) is better in forecasting than a monthly one, so perhaps some changes could be done to forecast even better results (though my laptop could not handle the heavy computation).
